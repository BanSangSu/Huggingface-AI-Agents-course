{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BanSangSu/Huggingface-AI-Agents-course/blob/main/unit2/langgraph/agent_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "89791f21c171372a"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent\n",
        "\n",
        "In this notebook, **we're going to build a simple agent using using LangGraph**.\n",
        "\n",
        "This notebook is part of the <a href=\"https://www.hf.co/learn/agents-course\">Hugging Face Agents Course</a>, a free course from beginner to expert, where you learn to build Agents.\n",
        "\n",
        "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
        "\n",
        "As seen in the Unit 1, an agent needs 3 steps as introduced in the ReAct architecture :\n",
        "[ReAct](https://react-lm.github.io/), a general agent architecture.\n",
        "\n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "\n",
        "\n",
        "![Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Agent.png)"
      ],
      "id": "89791f21c171372a"
    },
    {
      "metadata": {
        "id": "bef6c5514bd263ce"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "%pip install -q -U langchain_openai langchain_core langgraph"
      ],
      "id": "bef6c5514bd263ce"
    },
    {
      "metadata": {
        "id": "61d0ed53b26fa5c6"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import os\n",
        "\n",
        "# Please setp your own key.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxx\""
      ],
      "id": "61d0ed53b26fa5c6"
    },
    {
      "metadata": {
        "id": "a4a8bf0d5ac25a37"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "vision_llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "\n",
        "def extract_text(img_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract text from an image file using a multimodal model.\n",
        "\n",
        "    Args:\n",
        "        img_path: A local image file path (strings).\n",
        "\n",
        "    Returns:\n",
        "        A single string containing the concatenated text extracted from each image.\n",
        "    \"\"\"\n",
        "    all_text = \"\"\n",
        "    try:\n",
        "\n",
        "        # Read image and encode as base64\n",
        "        with open(img_path, \"rb\") as image_file:\n",
        "            image_bytes = image_file.read()\n",
        "\n",
        "        image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "\n",
        "        # Prepare the prompt including the base64 image data\n",
        "        message = [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": (\n",
        "                            \"Extract all the text from this image. \"\n",
        "                            \"Return only the extracted text, no explanations.\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{image_base64}\"\n",
        "                        },\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Call the vision-capable model\n",
        "        response = vision_llm.invoke(message)\n",
        "\n",
        "        # Append extracted text\n",
        "        all_text += response.content + \"\\n\\n\"\n",
        "\n",
        "        return all_text.strip()\n",
        "    except Exception as e:\n",
        "        # You can choose whether to raise or just return an empty string / error message\n",
        "        error_msg = f\"Error extracting text: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "tools = [\n",
        "    divide,\n",
        "    extract_text\n",
        "]\n",
        "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
      ],
      "id": "a4a8bf0d5ac25a37"
    },
    {
      "metadata": {
        "id": "3e7c17a2e155014e"
      },
      "cell_type": "markdown",
      "source": [
        "Let's create our LLM and prompt it with the overall desired agent behavior."
      ],
      "id": "3e7c17a2e155014e"
    },
    {
      "metadata": {
        "id": "f31250bc1f61da81"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from typing import TypedDict, Annotated, Optional\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    # The input document\n",
        "    input_file: Optional[str]  # Contains file path, type (PNG)\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "id": "f31250bc1f61da81"
    },
    {
      "metadata": {
        "id": "3c4a736f9e55afa9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "def assistant(state: AgentState):\n",
        "    # System message\n",
        "    textual_description_of_tool = \"\"\"\n",
        "extract_text(img_path: str) -> str:\n",
        "    Extract text from an image file using a multimodal model.\n",
        "\n",
        "    Args:\n",
        "        img_path: A local image file path (strings).\n",
        "\n",
        "    Returns:\n",
        "        A single string containing the concatenated text extracted from each image.\n",
        "divide(a: int, b: int) -> float:\n",
        "    Divide a and b\n",
        "\"\"\"\n",
        "    image = state[\"input_file\"]\n",
        "    sys_msg = SystemMessage(content=f\"You are an helpful agent that can analyse some images and run some computatio without provided tools :\\n{textual_description_of_tool} \\n You have access to some otpional images. Currently the loaded images is : {image}\")\n",
        "\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])], \"input_file\": state[\"input_file\"]}"
      ],
      "id": "3c4a736f9e55afa9"
    },
    {
      "metadata": {
        "id": "6f1efedd943d8b1d"
      },
      "cell_type": "markdown",
      "source": [
        "We define a `tools` node with our list of tools.\n",
        "\n",
        "The `assistant` node is just our model with bound tools.\n",
        "\n",
        "We create a graph with `assistant` and `tools` nodes.\n",
        "\n",
        "We add `tools_condition` edge, which routes to `End` or to `tools` based on  whether the `assistant` calls a tool.\n",
        "\n",
        "Now, we add one new step:\n",
        "\n",
        "We connect the `tools` node *back* to the `assistant`, forming a loop.\n",
        "\n",
        "* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.\n",
        "* If it is a tool call, the flow is directed to the `tools` node.\n",
        "* The `tools` node connects back to `assistant`.\n",
        "* This loop continues as long as the model decides to call tools.\n",
        "* If the model response is not a tool call, the flow is directed to END, terminating the process."
      ],
      "id": "6f1efedd943d8b1d"
    },
    {
      "metadata": {
        "id": "e013061de784638a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "id": "e013061de784638a"
    },
    {
      "metadata": {
        "id": "d3b0ba5be1a54aad"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
        "\n",
        "messages = react_graph.invoke({\"messages\": messages, \"input_file\": None})"
      ],
      "id": "d3b0ba5be1a54aad"
    },
    {
      "metadata": {
        "id": "55eb0f1afd096731"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "55eb0f1afd096731"
    },
    {
      "metadata": {
        "id": "e0062c1b99cb4779"
      },
      "cell_type": "markdown",
      "source": [
        "## Training program\n",
        "MR Wayne left a note with his training program for the week. I came up with a recipe for dinner left in a note.\n",
        "\n",
        "you can find the document [HERE](https://huggingface.co/datasets/agents-course/course-images/blob/main/en/unit2/LangGraph/Batman_training_and_meals.png), so download it and upload it in the local folder.\n",
        "\n",
        "![Training](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Batman_training_and_meals.png)"
      ],
      "id": "e0062c1b99cb4779"
    },
    {
      "metadata": {
        "id": "2e166ebba82cfd2a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "messages = [HumanMessage(content=\"According the note provided by MR wayne in the provided images. What's the list of items I should buy for the dinner menu ?\")]\n",
        "\n",
        "messages = react_graph.invoke({\"messages\": messages, \"input_file\": \"Batman_training_and_meals.png\"})"
      ],
      "id": "2e166ebba82cfd2a"
    },
    {
      "metadata": {
        "id": "5bfd67af70b7dcf3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "id": "5bfd67af70b7dcf3"
    },
    {
      "metadata": {
        "id": "8cd664ab5ee5450e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "8cd664ab5ee5450e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice"
      ],
      "metadata": {
        "id": "loErCrXBQzvh"
      },
      "id": "loErCrXBQzvh"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain_openai langchain_core langgraph"
      ],
      "metadata": {
        "id": "KvNuBWNBQ0pf"
      },
      "id": "KvNuBWNBQ0pf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "PR0tQoF-ZI67"
      },
      "id": "PR0tQoF-ZI67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "vision_llm = ChatOpenAI(\n",
        "    model_name=\"meta-llama/llama-4-maverick:free\",  # 또는 원하는 모델명\n",
        "    # openai_api_key=\"YOUR_OPENROUTER_API_KEY\",  # API Key\n",
        "    base_url=\"https://openrouter.ai/api/v1\",  # or openai_api_base\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"deepseek/deepseek-chat-v3-0324:free\",  # 또는 원하는 모델명\n",
        "    # openai_api_key=\"YOUR_OPENROUTER_API_KEY\",  # API Key\n",
        "    base_url=\"https://openrouter.ai/api/v1\",  # or openai_api_base\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "PVbdTHszUpGk"
      },
      "id": "PVbdTHszUpGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Agent’s State"
      ],
      "metadata": {
        "id": "JSTEJRn1d3tR"
      },
      "id": "JSTEJRn1d3tR"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, Optional\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    input_file: Optional[str]\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "wLYZwwzLdFzg"
      },
      "id": "wLYZwwzLdFzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Tools"
      ],
      "metadata": {
        "id": "QZmNRWsVd6Vr"
      },
      "id": "QZmNRWsVd6Vr"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(img_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract text from an image file using a multimodal model.\n",
        "\n",
        "    Master Wayne often leaves notes with his training regimen or meal plans.\n",
        "    This allows me to properly analyze the contents.\n",
        "    \"\"\"\n",
        "    all_text = \"\"\n",
        "    try:\n",
        "        ## Note.\n",
        "        ## The Llama-4-Maverick model on OpenRouter does not accept images as base64 data; instead, it requires image inputs to be provided as URLs that are accessible via the web.\n",
        "\n",
        "\n",
        "        # with open(img_path, \"rb\") as image_file: # not for llama-4-maverick\n",
        "        #     image_bytes = image_file.read()\n",
        "\n",
        "        # image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\") # not for llama-4-maverick\n",
        "\n",
        "        message = [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": (\n",
        "                            \"Extract all the text from this image. \"\n",
        "                            \"Return only the extracted text, no explanations.\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            # \"url\": f\"data:image/png;base64,{image_base64}\" # not for llama-4-maverick\n",
        "                            \"url\": img_path\n",
        "                        },\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        response = vision_llm.invoke(message)\n",
        "\n",
        "        all_text += response.content + \"\\n\\n\"\n",
        "\n",
        "        return all_text.strip()\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error extracting text: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b - for Master Wayne's occasional calculations.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "tools = [\n",
        "    divide,\n",
        "    extract_text\n",
        "]\n",
        "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
      ],
      "metadata": {
        "id": "P1d5igped6p6"
      },
      "id": "P1d5igped6p6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The nodes"
      ],
      "metadata": {
        "id": "Gl9X_JY-d7yf"
      },
      "id": "Gl9X_JY-d7yf"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "def assistant(state: AgentState):\n",
        "    textual_description_of_tool = \"\"\"\n",
        "extract_text(img_path: str) -> str:\n",
        "    Extract text from an image file using a multimodal model.\n",
        "\n",
        "    Args:\n",
        "        img_path: A local image file path (strings).\n",
        "\n",
        "    Returns:\n",
        "        A single string containing the concatenated text extracted from each image.\n",
        "divide(a: int, b: int) -> float:\n",
        "    Divide a and b\n",
        "\"\"\"\n",
        "    image=state[\"input_file\"]\n",
        "    sys_msg = SystemMessage(content=f\"You are an helpful agent that can analyse some images and run some computatio without provided tools :\\n{textual_description_of_tool} \\n You have access to some otpional images. Currently the loaded images is : {image}\")\n",
        "\n",
        "    return {\n",
        "        \"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])],\n",
        "        \"input_file\": state[\"input_file\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "ImmcZyfLd8IL"
      },
      "id": "ImmcZyfLd8IL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The ReAct Pattern: How I Assist Mr. Wayne"
      ],
      "metadata": {
        "id": "good6HEud-uY"
      },
      "id": "good6HEud-uY"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from IPython.display import Image, display\n",
        "\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "mbRCrlH9d_CO"
      },
      "id": "mbRCrlH9d_CO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Butler in Action (divide)"
      ],
      "metadata": {
        "id": "SThHfxcweBnv"
      },
      "id": "SThHfxcweBnv"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
        "\n",
        "messages = react_graph.invoke({\"messages\": messages, \"input_file\": None})"
      ],
      "metadata": {
        "id": "ca9rcp5VeB5L"
      },
      "id": "ca9rcp5VeB5L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "IKmaXmRmwY5z"
      },
      "id": "IKmaXmRmwY5z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training program (vision task (VLM))"
      ],
      "metadata": {
        "id": "mkcNzckuw1gg"
      },
      "id": "mkcNzckuw1gg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image file from url"
      ],
      "metadata": {
        "id": "CiTspdICx9PR"
      },
      "id": "CiTspdICx9PR"
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# image_url = \"https://huggingface.co/datasets/agents-course/course-images/blob/main/en/unit2/LangGraph/Batman_training_and_meals.png\"\n",
        "# image_name = \"Batman_training_and_meals.png\"\n",
        "\n",
        "# Download the image\n",
        "# urllib.request.urlretrieve(image_url, image_name)\n",
        "\n",
        "image_url = \"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Batman_training_and_meals.png\""
      ],
      "metadata": {
        "id": "Z_BBX_mKwnVN"
      },
      "id": "Z_BBX_mKwnVN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu?\")]\n",
        "# messages = react_graph.invoke({\"messages\": messages, \"input_file\": image_name})\n",
        "messages = react_graph.invoke({\"messages\": messages, \"input_file\": image_url})"
      ],
      "metadata": {
        "id": "xDIKMwRCx1jP"
      },
      "id": "xDIKMwRCx1jP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "qzpgBYjcyas_"
      },
      "id": "qzpgBYjcyas_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "89791f21c171372a"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}